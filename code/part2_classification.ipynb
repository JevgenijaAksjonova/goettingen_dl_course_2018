{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST classification\n",
    "\n",
    "In this notebook we tackle the perhaps most well known problem in all of machine learning, classifying hand-written digits. In particular, we will use the MNIST (Modified National Institute of Standards and Technology) dataset. The digits are 28x28 pixel images that look somewhat like this:\n",
    "\n",
    "![](https://user-images.githubusercontent.com/2202312/32365318-b0ccc44a-c079-11e7-8fb1-6b1566c0bdc4.png)\n",
    "\n",
    "Each digit has been hand classified, e.g. for the above 9-7-0-9-0-...\n",
    "\n",
    "Our task is to teach a neural network to perform this classification, i.e. we want to find a function $\\mathcal{T}_\\theta$ such that\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "|$\\mathcal{T}_\\theta$(|<img align=\"center\" src=\"https://user-images.githubusercontent.com/2202312/33177374-b134e572-d062-11e7-87c7-0574c6f5bee9.png\" width=\"28\"/>|) = 4|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies\n",
    "\n",
    "This should run without errors if all dependencies are installed properly. However you may get some warnings, these are typically safe to ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tensorflow\n",
    "\n",
    "In these exercies we'll use the software library [Tensorflow](https://www.tensorflow.org/), which is a state of the art library for numerical computations in python developed by Google. The package supports computation on both the CPU and GPU, and also supports automatic differentiation (more on that later).\n",
    "\n",
    "### Execution\n",
    "Writing code in tensorflow is quite similar to classical python, except for one thing: tensorflow uses *deferred execution*, this means that instead of executing the code in one step (called eager execution), the computational graph is first defined, and it is then fed data upon which the computation is performed. Users from compiled languages will recongnize this execution method.\n",
    "\n",
    "The execution flow typically goes like this:\n",
    "\n",
    "* Define placeholders for the input\n",
    "* Define what computations should be performed\n",
    "* Feed the placeholders data\n",
    "\n",
    "In small examples like this, this may seem very cumbersome. The computational model has several upsides that are sadly not evident in simple samples like these, including parallelization, packaging and optimization. However once you start working on larger examples these become more evident.\n",
    "\n",
    "### Name scopes\n",
    "Tensorflow also uses scopes (e.g. [`tf.name_scope`](https://www.tensorflow.org/api_docs/python/tf/name_scope)) to organize the code. This can interact very nicely with external tools like [tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard), but is also used as comments in the code and to give better error messages.\n",
    "\n",
    "### Batched computation\n",
    "Finally, tensorflow (and most other deep learning libraries) uses batched execution. This means that instead of computing the result one element at a time, or one image at a time, the result is computed simultaneously for a *batch* of images, e.g. several. The batch size is typically in the range of 1 to 512, depending on the input size. This is done in order to enable faster computations on modern hardware. Due to this, the shape of objects used with tensorflow is typically\n",
    "\n",
    "```\n",
    "[BATCH_SIZE, ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sessions\n",
    "\n",
    "Tensorflow uses *sessions* to encapuslate computations. These can be useful when running on e.g. multiple GPUs. For simple examples like this, an `InteractiveSession` works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start a tensorflow session\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Set the random seed to enable reproducible code\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST data\n",
    "\n",
    "We now need to get the data we will use, which in this case is the famous [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, a set of 65000 hand-written digits.\n",
    "\n",
    "\n",
    "In order to not cheat, it is best practice in machine learning to train on one set (conveniently called the \"train\" set) and evaluate on another, called the \"test\"-set. If the method performs well on the test dataset, we can expect it to perform well on other unseen datasets as well.\n",
    "\n",
    "In the standard split of the MNIST data, 55000 datapoints are used for training and 10000 for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MNIST data\n",
    "\n",
    "Luckily for us, there is an easy to use tool for working the MNIST dataset in `tensorflow.examples.tutorials.mnist.input_data` that we previously imported.\n",
    "\n",
    "This utility downloads the data if needed and also provides utilities for accessing the training and testing sets as well as for getting small minibatches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data\n",
    "\n",
    "In machine learning, the first step is **always** to try to understand your data. How does it look? What pre-processing has been done?\n",
    "\n",
    "In this case we can [read the documentation](http://yann.lecun.com/exdb/mnist/) to find:\n",
    "\n",
    "* The images were rescaled to size 20 x 20\n",
    "* The numbers are all fit in a 28 x 28 square and centered according to center of mass $\\approx [14, 14]$\n",
    "* the values span exactly $[0, 1]$.\n",
    "* The numbers are from $\\approx 250$ writers and scrambled in order\n",
    "\n",
    "In general, documentation is always feeble (perhaps the data has been modified in our data loader?), and it may be a good idea to verify these statements yourself. To do this, we look at the first datapoint in the test set. It is labeled as a \"7\", and visually this seems to be true. Likewise, the center of mass does indeed seem to be at $\\approx [14, 14]$. We also note that the values are *inverted* (white on black background)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center of mass = (14.383709839842643, 14.340846509159318)\n",
      "min value = 0.0, max value = 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUv8Hf5v6lCN/uH4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22349935080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = mnist.test.images[0].reshape(28, 28)\n",
    "value =  mnist.test.labels[0]\n",
    "print('center of mass = {}'.format(scipy.ndimage.measurements.center_of_mass(img)))\n",
    "print('min value = {}, max value = {}'.format(np.min(img), np.max(img)))\n",
    "plt.imshow(img, cmap='Greys_r');\n",
    "plt.title('digit = {}'.format(value))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function\n",
    "\n",
    "Before we start defining methods, we must first settle on a definition of what it meas for a method to be good. Doing this is non-trivial for a range of tasks, and depending on our choice of \"goodness\" we may obtain different \"best\" methods.\n",
    "\n",
    "In the setting of classification, we would ideally like to learn a function $f$ such that $f(x) = n$ where $x$ is the image and $n$ the corresponding digit, and we would like this to hold for *any* image. Such a loss function could be written\n",
    "$$\n",
    "    L(f) = \\int I(f(x) \\neq n) d\\mu(x, n)\n",
    "$$\n",
    "where $\\mu(x, n)$ is the probability density of images $x$ and labels $n$, and $I$ is the indicator function.\n",
    "\n",
    "However, we only have access to the training set (55000 images/labels) and thus there is no way for us compute the integral. Hence we will use the Empirical Risk Minimization (ERM) paradigm, wherein we assume that we have Independent Identically Distributed (I.I.D.) samples $(x_i, n_i)$ from $\\mu$ and define the best function by minimization of the empirical risk\n",
    "$$\n",
    "    \\hat{L}(f) = \\frac{1}{N} \\sum_{i = 1}^N I(f(x_i) \\neq n_i)\n",
    "$$\n",
    "\n",
    "Finally, even if our function $f$ is good on the training set, we do not know if it is good on unseen data (e.g. we don't know if it generalizes). For example, the function\n",
    "$$\n",
    "    f(x) =\n",
    "    \\begin{cases}\n",
    "        n_i & \\text{ if } x = x_i\\\\\n",
    "        0 & \\text{ else.}\n",
    "    \\end{cases}\n",
    "$$\n",
    "has 0 empirical risk, but is obviously a really bad classifier (it is correct with probability zero).\n",
    "\n",
    "To validate that our classifier works on unseen data, we also apply it to the test dataset. The function below computes the empirical test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all the images and labels from the test set\n",
    "batch = mnist.test.next_batch(10000)\n",
    "test_images = batch[0].reshape([-1, 28, 28, 1])\n",
    "test_labels = batch[1]\n",
    "\n",
    "def test_error(result_tensor, data_placeholder):\n",
    "    \"\"\"Evaluate a reconstruction method by computing the % correct.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    result_tensor : `tf.Tensor`, shape (None,)\n",
    "        The tensorflow tensor containing the result of the classification.\n",
    "    data_placeholder : `tf.Tensor`, shape (None, 28, 28, 1) or (None, 784)\n",
    "        The tensorflow tensor containing the input to the classification operator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MSE : float\n",
    "        Mean squared error of the reconstruction.\n",
    "    \"\"\"\n",
    "    feed_images = np.reshape(test_images, [-1, *data_placeholder.shape[1:]])\n",
    "    result = result_tensor.eval(\n",
    "        feed_dict={data_placeholder: feed_images})\n",
    "\n",
    "    return np.mean(result == test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, the loss function was defined for *any* classifier $f$, but for computational feasibility we typically need to constrain our selves to some set of functions parametrized by some parameters $\\{f_\\theta\\}_{\\theta \\in \\Theta}$.\n",
    "\n",
    "The most simple nontrivial method is the linear regression\n",
    "$$\n",
    "    f_\\theta(x)\n",
    "    =\n",
    "    \\text{argmax}_{i \\in \\{0, \\dots, 9\\}}\n",
    "    \\big(\\langle w_i, x \\rangle + b_i\\big)\n",
    "$$\n",
    "where $\\theta = (w_0, \\dots w_9, b_0, \\dots, b_9)$, $w_i \\in \\mathbb{R}^{28 \\times 28}$ and $b_i \\in \\mathbb{R}$.\n",
    "\n",
    "However, even for this very simple classifier, we find that solving the optimization problem $\\min \\hat{L}(\\theta)$ is  basically impossible, and this is since we cannot compute the gradient\n",
    "$$\n",
    "    \\frac{\\partial \\hat{L}(\\theta)}{\\partial \\theta}\n",
    "$$\n",
    "and this problem does in fact apply to any loss function that involves the exact classification error.\n",
    "\n",
    "A solution to this is to forfeit the idea of minimizing the classification error and instead minimize some *surrogate*. An exceedingly popular such surrogate is to compute a probability distribution for each image. For example, for the linear regression above we could define the probability of image $x$ corresponding to the number $n$ by\n",
    "$$\n",
    "f_\\theta(x, n) = \\frac{\\exp\\big(\\langle w_n, x \\rangle + b_n\\big)}{\\sum_{j=0}^9 \\exp\\big(\\langle w_j, x \\rangle + b_j\\big)}\n",
    "$$\n",
    "However, the question then becomes: what is a good loss function? Since we now have functions that return probabilities, a good choice of a loss function would be to minimize the some probability distance between the estimated probabilities and the true probabilities.\n",
    "\n",
    "One such metric is the Kullback-Leibler distance which gives\n",
    "\\\\[\n",
    "L(\\theta) = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{n=0}^9 I(n_i = n) \\log(f_\\theta(x_i, n))\n",
    "\\\\]\n",
    "Regression with this method is called [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), and it is perhaps the most well known and widely applied classification method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementary Implementation of logistic regression\n",
    "\n",
    "We start with an elementary implementation of logistic regression in `TensorFlow`. This is to show how `TensorFlow` works \"under the hood\", and does *not* reflect how one typically writes tensorflow code.\n",
    "\n",
    "Here there are a few components we'll use:\n",
    "\n",
    "* [`tf.placeholder`](https://www.tensorflow.org/api_docs/python/tf/placeholder) is as it sounds a placeholder for some value we will later provide. Here we use `shape=(None, 784)` where `None` indicates that this value is not a-priori determined, we can use any number. $784$ is simply $28 \\times 28$.\n",
    "* [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) is something (in fact the only thing) that can change its value over time. This will be there parameters $\\theta$ that we minimize over. We also provide initial values, normally distributed for the weights and zero for the bias.\n",
    "\n",
    "We then compute the logarithmic probabilities according to the formulae above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('elementary_network'):\n",
    "    # Create a placeholder for our input data (no computation is done here)\n",
    "    inp = tf.placeholder(shape=(None, 784), dtype=tf.float32, name=\"input\")\n",
    "    \n",
    "    # Create the parameters (weight, bias) of the model\n",
    "    weights = tf.Variable(tf.random_normal((784, 10)), name=\"weights\")\n",
    "    bias = tf.Variable(tf.zeros((10)), name=\"bias\")\n",
    "    \n",
    "    # Compute the probabilities (this is all lazy, no computations are actually performed)\n",
    "    lin = tf.matmul(inp, weights) + bias\n",
    "    elin = tf.exp(lin)\n",
    "    Z = tf.reduce_sum(elin, axis=1, keep_dims=True)\n",
    "    prob = elin / Z\n",
    "    log_prob = tf.log(prob)\n",
    "    \n",
    "    # Prediction by the most likely digit\n",
    "    pred = tf.argmax(log_prob, axis=1)  # axis 0 is the batch, axis 1 is the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Define the loss function which measures how good our parameters are, recall:\n",
    "\\\\[\n",
    "    L(\\theta) = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{n=0}^9 I(n_i = n) \\log(f_\\theta(x_i, n))\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to convert from labels to probabilities, we use the one-hot-encoding, which can be seen as computing the true probabilities of each digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toh = tf.one_hot([1, 1, 2], depth=10)\n",
    "toh.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"elementary_loss\"):\n",
    "    labels = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "    determ = tf.one_hot(labels, depth=10)\n",
    "    loss = -tf.reduce_mean(tf.reduce_sum(determ * log_prob, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step\n",
    "\n",
    "We'll train the network using gradient descent, i.e.\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\omega \\nabla L(\\theta)$$\n",
    "\n",
    "where $\\omega$ is the *learning rate*, or step size.\n",
    "\n",
    "Note that in machine learning, we typically use *stochastic* gradient descent (SGD). In these methods we don't use all of the data to compute the gradient, only a small subset called a minibatch. Here we use 128 images in each minibatch.\n",
    "\n",
    "Further, while for this case computing the gradient would be quite simple, once we move to harder and more complicated models doing so would be basically impossible to do by hand. To work around this, all major deep learning frameworks implement [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). This may sound fancy, but automatic differentiation is simply the chain rule for the adjoint of the derivative derivative:\n",
    "\n",
    "$$\n",
    "[D(g \\circ f)(x)]^*(y) = [Df(x)]^*\\big([Dg(f(x))]^*(y)\\bigr)\n",
    "$$\n",
    "\n",
    "Tensorflow implements it using the [`tf.gradients`](https://www.tensorflow.org/api_docs/python/tf/gradients) command.\n",
    "\n",
    "As noted above, `tf.Variable`s are the only mutable objects in tensorflow and we'll use that to update them in place using the `var.assign` method. Note that we still do not actually perform this operation, we're only building the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"elementary_training\"):\n",
    "    learning_rate = .1\n",
    "\n",
    "    variables = [weights, bias]\n",
    "    gradients = tf.gradients(loss, variables)\n",
    "    update_ops = [var.assign(var - learning_rate*grad) \n",
    "                  for var, grad in zip(variables, gradients)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Since all the code above was lazy, nothing has actually happened. Before we start we need to initialize the variables. This causes the (normally distributed) initializer of `weights` to run, setting its initial values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training\n",
    "\n",
    "We train the network by feeding data from the training set and occationally evalute the performance on our test set, this is the first point we actually start doing computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.6%, 84.4%, 86.0%, 87.0%, 87.5%, 87.8%, 88.1%, 88.5%, 88.7%, 89.0%, "
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    inp_, labels_ = mnist.train.next_batch(128)\n",
    "    session.run(update_ops, \n",
    "                feed_dict={labels:labels_, inp:inp_})\n",
    "    \n",
    "    if i % 1000 == 999:\n",
    "        print(\"{:.1f}%, \".format(test_error(pred, inp)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorFlow libraries\n",
    "\n",
    "While the above code solves our problem, it involved several small and perhaps obscure steps. Once we start moving to more complicated neural networks the code would become very repetetive.\n",
    "\n",
    "Since all of the steps are standardized, we can (and should) instead use built in tensorflow functions, this example does that, and all following examples will do the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "The \"network\" can be computed using the [`tf.contrib.layers.fully_connected`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected) function which computes\n",
    "\n",
    "$$\\rho(Wx + b)$$\n",
    "\n",
    "where $\\rho$ is the activation function, $W$ is a matrix called the weights and $b$ is a real vector called the bias. Note that here we never explicitly construct these, they are hidden inside tensorflow.\n",
    "\n",
    "Also remember that above we needed to give an initializer to the weights and biases, this is now done automatically (but can be modified via the `weights_initializer` and `bias_initializer` parameters.) However in this case we'll just use defaults. If we check the defaults, this means\n",
    "\n",
    "* `weights_initializer=`[`tf.contrib.layers.xavier_initializer`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/xavier_initializer). This is a rather smart initializer, which basically choses the weights such that the norm of the input and output is approximately equal, which gives more stable training.\n",
    "* `bias_initializer=`[`tf.zeros_initializer`](https://www.tensorflow.org/api_docs/python/tf/zeros_initializer). Which does exactly what you expect it to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('logistic_regression'):\n",
    "    logits = tf.contrib.layers.fully_connected(inp, \n",
    "                                               num_outputs=10,      # We only need to specify the number of outputs\n",
    "                                               activation_fn=None)  # Dont use any activation (include in loss instead)\n",
    "    pred = tf.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and optimization\n",
    "\n",
    "The loss function defined above should be done using the [`tf.nn.softmax_cross_entropy_with_logits`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits) function, which not only is easier to use, it is also more numerically stable (once again a reason to use libraries).\n",
    "\n",
    "In addition to this, we don't really have to write our own optimizer, and there are in fact several very good optimizers built into tensorflow. In this example we'll use [`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) which is a very popular and efficient gradient-based optimizer. All of the details regarding weights etc are hidden inside tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    one_hot_labels = tf.one_hot(labels, depth=10)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels,\n",
    "                                                   logits=logits)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network\n",
    "\n",
    "Training the network looks about the same as above except this is now much faster due to better initialization and because we use a better optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.5%, 92.2%, 92.3%, 92.6%, 92.6%, 92.8%, 92.6%, 92.8%, 92.7%, 92.7%, "
     ]
    }
   ],
   "source": [
    "# Initialize all TF variables\n",
    "for i in range(10000):\n",
    "    inp_, labels_ = mnist.train.next_batch(128)\n",
    "    session.run(optimizer, \n",
    "                feed_dict={labels:labels_, inp:inp_})\n",
    "\n",
    "    if i % 1000 == 999:\n",
    "        print(\"{:.1f}%, \".format(test_error(pred, inp)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "The first \"deep\" neural networks were [multilayer perceptrons](https://en.wikipedia.org/wiki/Multilayer_perceptron). In these we have a function of the following form\n",
    "\n",
    "$$\n",
    "W_3\\rho(W_2\\rho(W_1 x + b_1) + b_2) + b_3\n",
    "$$\n",
    "\n",
    "Where $W_i \\in \\mathbb{R}^{n_{i - 1} \\times n_{i}}$ are matrices and $b_i \\in \\mathbb{R}^{n_{i}}$ vectors and $\\rho$ a pointwise nonlinearity $\\mathbb{R}^{n_{i}} \\to \\mathbb{R}^{n_{i}}$.\n",
    "\n",
    "Note that the logistic regression can be cast into this form (how?).\n",
    "\n",
    "In this very simple neural network we'll use the following parameters:\n",
    "\n",
    "* Network size:\n",
    "  * `n_0=784` ($28 \\times 28$)\n",
    "  * `n_1=128`\n",
    "  * `n_2=32`\n",
    "  * `n_3=10` (number of labels)\n",
    "* ReLU nonlinearities $\\rho$\n",
    "$$\n",
    "ReLU(x) = \\max(x, 0)\n",
    "$$\n",
    "\n",
    "Thanks to the utilities defined above implementing this network is very straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.0%, 97.4%, 97.7%, 97.7%, 97.6%, 97.8%, 97.1%, 97.5%, 97.8%, 97.7%, "
     ]
    }
   ],
   "source": [
    "with tf.name_scope('logistic_regression'):\n",
    "    x = tf.contrib.layers.fully_connected(inp, num_outputs=128)  # the default activation function is ReLU\n",
    "    x = tf.contrib.layers.fully_connected(x, num_outputs=32)\n",
    "    logits = tf.contrib.layers.fully_connected(x, \n",
    "                                               num_outputs=10,\n",
    "                                               activation_fn=None)\n",
    "    pred = tf.argmax(logits, axis=1)\n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    one_hot_labels = tf.one_hot(labels, depth=10)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels,\n",
    "                                                   logits=logits)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Initialize all TF variables\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    inp_, labels_ = mnist.train.next_batch(128)\n",
    "    session.run(optimizer, \n",
    "                feed_dict={labels:labels_, inp:inp_})\n",
    "\n",
    "    if i % 1000 == 999:\n",
    "        print(\"{:.1f}%, \".format(test_error(pred, inp)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional network\n",
    "\n",
    "Convolutional neural networks are a corner-stone of the deep learning revolution. Here instead of using traditionall fully-connected layers which connect each point with all other points, we use spatial convolutions instead. By doing this, we get a translation invariant operator that acts locally. \n",
    "\n",
    "A convolution operator $C : \\mathcal{X}^n \\to \\mathcal{X}^m$, where $\\mathcal{X}$ is some set of images and $n$ is called the number of channels, is defined by\n",
    "$$\n",
    "[C(x)]_i = \\sum_j (w_{ij} \\ast x_j + b_j)\n",
    "$$\n",
    "where the *weight* $w_{ij}$ is a convolution kernel and $b_j \\in \\mathbb{R}$ is the bias.\n",
    "\n",
    "In applications, there number of duplicate entries $n, m$ is typically $\\in [32, 512]$ and the convolution kernel is given by a small $e.g. 3 \\times 3$ stencil. We'll use the built in function [`tf.contrib.layers.conv2d`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/conv2d) in order to perform the convolutions efficiently. Specifically this function operates on arrays with shape `[batch, height, width, channels]`, which is called BHWC order. Do note that this is not universal and e.g. PyTorch uses BCHW order.\n",
    "\n",
    "In order to get non-local behaviour we stack several of these on top of each other in the same manner as a MLP.\n",
    "\n",
    "The following code is a very simplified convolutional neural network for digit classification with the following layers\n",
    "\n",
    "* `3x3` convolution with 32 channels and and ReLU nonlinearity.\n",
    "* `2x2` maxpooling to reduce the dimension.\n",
    "* `3x3` convolution with 32 channels and and ReLU nonlinearity.\n",
    "* `2x2` maxpooling.\n",
    "* Flatten the images and channels to a one-dimensional array\n",
    "* $1568 \\to 128$ fully connected layer with ReLU nonlinearity\n",
    "* $128 \\to 10$ fully connected layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.0%, 98.6%, 98.4%, 98.7%, 98.9%, 99.1%, 99.0%, 98.9%, 99.1%, 98.9%, "
     ]
    }
   ],
   "source": [
    "with tf.name_scope('convolutional_network'):\n",
    "    # Reshape the input from [?, 784] to [?, 28, 28, 1] where 1 is the number of channels (e.g. n above)\n",
    "    images = tf.reshape(inp, [-1, 28, 28, 1])\n",
    "    \n",
    "    x = tf.contrib.layers.conv2d(images, \n",
    "                                 num_outputs=32, # Number of \"channels\", e.g. duplicates of the image\n",
    "                                 kernel_size=3)  # size of the convolution kernel\n",
    "    x = tf.contrib.layers.max_pool2d(x, \n",
    "                                     kernel_size=[2, 2])  # Size of strides\n",
    "    # x.shape = [?, 14, 14, 32]\n",
    "    x = tf.contrib.layers.conv2d(x, num_outputs=32, kernel_size=3)\n",
    "    x = tf.contrib.layers.max_pool2d(x, kernel_size=[2, 2])\n",
    "    # x.shape = [?, 7, 7, 32]\n",
    "    x = tf.contrib.layers.flatten(x)    \n",
    "    # x.shape = [?, 1568]\n",
    "    \n",
    "    # It is typically a good idea to finish with fully connected layers \n",
    "    # in order to encode the non-translation invariant information\n",
    "    x = tf.contrib.layers.fully_connected(x, 128)\n",
    "    logits = tf.contrib.layers.fully_connected(x, 10,\n",
    "                                               activation_fn=None)\n",
    "    pred = tf.argmax(logits, axis=1)\n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    one_hot_labels = tf.one_hot(labels, depth=10)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels,\n",
    "                                                   logits=logits)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Initialize all TF variables\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    inp_, labels_ = mnist.train.next_batch(32)\n",
    "    session.run(optimizer, \n",
    "                feed_dict={labels:labels_, inp:inp_})\n",
    "\n",
    "    if i % 1000 == 999:\n",
    "        print(\"{:.1f}%, \".format(test_error(pred, inp)*100), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "From this notebook there are some take-aways\n",
    "\n",
    "* A classification problem involves taking an image and assigning it to a class (e.g. 4)\n",
    "* Several methods from machine learning can be used for classificication\n",
    "* Software libraries like tensorflow are great\n",
    "* Deeper neural networks work very well\n",
    "* Convolutions allow us to encode spatial information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
